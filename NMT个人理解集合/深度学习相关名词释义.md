# 深度学习相关名词释义

## 激活函数（activation function）

### 效果

通过一个非线性的激活函数，把原先神经元作出的线性变换，转换成一个非线性变换，使原先复杂的线性组合变成了平滑的曲线，使得神经网络的表示能力更强，更好地拟合目标函数

### 几种常用激活函数

1. Sigmoid

   <a href="https://www.codecogs.com/eqnedit.php?latex=sigmoid(x)=\frac{1}{1&plus;e^{-x}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?sigmoid(x)=\frac{1}{1&plus;e^{-x}}" title="sigmoid(x)=\frac{1}{1+e^{-x}}" /></a>

2. tanh

   <a href="https://www.codecogs.com/eqnedit.php?latex=tanh(x)=&space;\frac{e^{x}-e^{-x}}{e^{x}&plus;e^{-x}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?tanh(x)=&space;\frac{e^{x}-e^{-x}}{e^{x}&plus;e^{-x}}" title="tanh(x)= \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}" /></a>

3. ReLU

   <a href="https://www.codecogs.com/eqnedit.php?latex=ReLU(x)=&space;max(0,x)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?ReLU(x)=&space;max(0,x)" title="ReLU(x)= max(0,x)" /></a>

### 激活函数的使用

对于一个神经元<a href="https://www.codecogs.com/eqnedit.php?latex=y=wx&plus;b" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y=wx&plus;b" title="y=wx+b" /></a>

在获得神经元本身的输出y后，再对其套用激活函数<a href="https://www.codecogs.com/eqnedit.php?latex={y}'&space;=&space;activation(y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?{y}'&space;=&space;activation(y)" title="{y}' = activation(y)" /></a>

这样